{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import copy, math\nimport numpy as np\ndef sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Parameters\n    ----------\n    z : array_like\n        A scalar or numpy array of any size.\n\n    Returns\n    -------\n     g : array_like\n         sigmoid(z)\n    \"\"\"\n    z = np.clip( z, -500, 500 )           # protect against overflow\n    g = 1.0/(1.0+np.exp(-z))\n\n    return g\n\n##########################################################\n# Regression Routines\n##########################################################\n\ndef predict_logistic(X, w, b):\n    \"\"\" performs prediction \"\"\"\n    return sigmoid(X @ w + b)\n\ndef predict_linear(X, w, b):\n    \"\"\" performs prediction \"\"\"\n    return X @ w + b\n\ndef compute_cost_logistic(X, y, w, b, lambda_=0, safe=False):\n    \"\"\"\n    Computes cost using logistic loss, non-matrix version\n\n    Args:\n      X (ndarray): Shape (m,n)  matrix of examples with n features\n      y (ndarray): Shape (m,)   target values\n      w (ndarray): Shape (n,)   parameters for prediction\n      b (scalar):               parameter  for prediction\n      lambda_ : (scalar, float) Controls amount of regularization, 0 = no regularization\n      safe : (boolean)          True-selects under/overflow safe algorithm\n    Returns:\n      cost (scalar): cost\n    \"\"\"\n\n    m,n = X.shape\n    cost = 0.0\n    for i in range(m):\n        z_i    = np.dot(X[i],w) + b                                             #(n,)(n,) or (n,) ()\n        if safe:  #avoids overflows\n            cost += -(y[i] * z_i ) + log_1pexp(z_i)\n        else:\n            f_wb_i = sigmoid(z_i)                                                   #(n,)\n            cost  += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)       # scalar\n    cost = cost/m\n\n    reg_cost = 0\n    if lambda_ != 0:\n        for j in range(n):\n            reg_cost += (w[j]**2)                                               # scalar\n        reg_cost = (lambda_/(2*m))*reg_cost\n\n    return cost + reg_cost\n\n\ndef log_1pexp(x, maximum=20):\n    ''' approximate log(1+exp^x)\n        https://stats.stackexchange.com/questions/475589/numerical-computation-of-cross-entropy-in-practice\n    Args:\n    x   : (ndarray Shape (n,1) or (n,)  input\n    out : (ndarray Shape matches x      output ~= np.log(1+exp(x))\n    '''\n\n    out  = np.zeros_like(x,dtype=float)\n    i    = x <= maximum\n    ni   = np.logical_not(i)\n\n    out[i]  = np.log(1 + np.exp(x[i]))\n    out[ni] = x[ni]\n    return out\n\n\ndef compute_cost_matrix(X, y, w, b, logistic=False, lambda_=0, safe=True):\n    \"\"\"\n    Computes the cost using  using matrices\n    Args:\n      X : (ndarray, Shape (m,n))          matrix of examples\n      y : (ndarray  Shape (m,) or (m,1))  target value of each example\n      w : (ndarray  Shape (n,) or (n,1))  Values of parameter(s) of the model\n      b : (scalar )                       Values of parameter of the model\n      verbose : (Boolean) If true, print out intermediate value f_wb\n    Returns:\n      total_cost: (scalar)                cost\n    \"\"\"\n    m = X.shape[0]\n    y = y.reshape(-1,1)             # ensure 2D\n    w = w.reshape(-1,1)             # ensure 2D\n    if logistic:\n        if safe:  #safe from overflow\n            z = X @ w + b                                                           #(m,n)(n,1)=(m,1)\n            cost = -(y * z) + log_1pexp(z)\n            cost = np.sum(cost)/m                                                   # (scalar)\n        else:\n            f    = sigmoid(X @ w + b)                                               # (m,n)(n,1) = (m,1)\n            cost = (1/m)*(np.dot(-y.T, np.log(f)) - np.dot((1-y).T, np.log(1-f)))   # (1,m)(m,1) = (1,1)\n            cost = cost[0,0]                                                       \n    else:\n        f    = X @ w + b                                                        # (m,n)(n,1) = (m,1)\n        cost = (1/(2*m)) * np.sum((f - y)**2)                                  \n\n    reg_cost = (lambda_/(2*m)) * np.sum(w**2)                                  \n\n    total_cost = cost + reg_cost                                               \n\n    return total_cost                                                           \n\ndef compute_gradient_matrix(X, y, w, b, logistic=False, lambda_=0):\n    \"\"\"\n    Computes the gradient using matrices\n\n    Args:\n      X : (ndarray, Shape (m,n))          matrix of examples\n      y : (ndarray  Shape (m,) or (m,1))  target value of each example\n      w : (ndarray  Shape (n,) or (n,1))  Values of parameters of the model\n      b : (scalar )                       Values of parameter of the model\n      logistic: (boolean)                 linear if false, logistic if true\n      lambda_:  (float)                   applies regularization if non-zero\n    Returns\n      dj_dw: (array_like Shape (n,1))     The gradient of the cost w.r.t. the parameters w\n      dj_db: (scalar)                     The gradient of the cost w.r.t. the parameter b\n    \"\"\"\n    m = X.shape[0]\n    y = y.reshape(-1,1)             # ensure 2D\n    w = w.reshape(-1,1)             # ensure 2D\n\n    f_wb  = sigmoid( X @ w + b ) if logistic else  X @ w + b      # (m,n)(n,1) = (m,1)\n    err   = f_wb - y                                              # (m,1)\n    dj_dw = (1/m) * (X.T @ err)                                   # (n,m)(m,1) = (n,1)\n    dj_db = (1/m) * np.sum(err)                                   # scalar\n\n    dj_dw += (lambda_/m) * w        # regularize                  # (n,1)\n\n    return dj_db, dj_dw                                           # scalar, (n,1)\n\ndef gradient_descent(X, y, w_in, b_in, alpha, num_epochs, logistic=False, lambda_=0, verbose=True):\n    \"\"\"\n    Performs batch gradient descent to learn theta. Updates theta by taking\n    num_epochs gradient steps with learning rate alpha\n\n    Args:\n      X (ndarray):    Shape (m,n)         matrix of examples\n      y (ndarray):    Shape (m,) or (m,1) target value of each example\n      w_in (ndarray): Shape (n,) or (n,1) Initial values of parameters of the model\n      b_in (scalar):                      Initial value of parameter of the model\n      logistic: (boolean)                 linear if false, logistic if true\n      lambda_:  (float)                   applies regularization if non-zero\n      alpha (float):                      Learning rate\n      num_epochs (int):                    number of iterations to run gradient descent\n\n\n    Returns:\n      w (ndarray): Shape (n,) or (n,1)    Updated values of parameters; matches incoming shape\n      b (scalar):                         Updated value of parameter\n    \"\"\"\n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    w = w.reshape(-1,1)      #prep for matrix operations\n    y = y.reshape(-1,1)\n\n    for i in range(num_epochs):\n\n        # Calculate the gradient and update the parameters\n        dj_db,dj_dw = compute_gradient_matrix(X, y, w, b, logistic, lambda_)\n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw\n        b = b - alpha * dj_db\n\n        # Save cost J at each iteration\n        if i<100000:      # prevent resource exhaustion\n            J_history.append( compute_cost_matrix(X, y, w, b, logistic, lambda_) )\n\n        # Print cost every at intervals 10 times or as many iterations if < 10\n        if i% math.ceil(num_epochs / 10) == 0:\n            if verbose: print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n\n    return w.reshape(w_in.shape), b, J_history  #return final w,b and J history for graphing\n\ndef zscore_normalize_features(X):\n    \"\"\"\n    computes  X, zcore normalized by column\n\n    Args:\n      X (ndarray): Shape (m,n) input data, m examples, n features\n\n    Returns:\n      X_norm (ndarray): Shape (m,n)  input normalized by column\n      mu (ndarray):     Shape (n,)   mean of each feature\n      sigma (ndarray):  Shape (n,)   standard deviation of each feature\n    \"\"\"\n    # find the mean of each column/feature\n    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n    # find the standard deviation of each column/feature\n    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n    # element-wise, subtract mu for that column from each example, divide by std for that column\n    X_norm = (X - mu) / sigma\n\n    return X_norm, mu, sigma\ndef compute_gradient_logistic(X, y, w, b): \n    \"\"\"\n    Computes the gradient for logistic regression \n \n    Args:\n      X (ndarray (m,n): Data, m examples with n features\n      y (ndarray (m,)): target values\n      w (ndarray (n,)): model parameters  \n      b (scalar)      : model parameter\n    Returns\n      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n    \"\"\"\n    m,n = X.shape\n    dj_dw = np.zeros((n,))                           #(n,)\n    dj_db = 0.\n\n    for i in range(m):\n        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n        err_i  = f_wb_i  - y[i]                       #scalar\n        for j in range(n):\n            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n        dj_db = dj_db + err_i\n    dj_dw = dj_dw/m                                   #(n,)\n    dj_db = dj_db/m                                   #scalar\n        \n    return dj_db, dj_dw  \ndef gradient_descent(X, y, w_in, b_in, alpha, num_epochs): \n    \"\"\"\n    Performs batch gradient descent\n    \n    Args:\n      X (ndarray (m,n)   : Data, m examples with n features\n      y (ndarray (m,))   : target values\n      w_in (ndarray (n,)): Initial values of model parameters  \n      b_in (scalar)      : Initial values of model parameter\n      alpha (float)      : Learning rate\n      num_epochs (scalar) : number of iterations to run gradient descent\n      \n    Returns:\n      w (ndarray (n,))   : Updated values of parameters\n      b (scalar)         : Updated value of parameter \n    \"\"\"\n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    \n    for i in range(num_epochs):\n        # Calculate the gradient and update the parameters\n        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw               \n        b = b - alpha * dj_db               \n      \n        # Save cost J at each epoch\n        if i<100000:      # prevent resource exhaustion \n            J_history.append( compute_cost_logistic(X, y, w, b) )\n\n        # Print cost every at intervals 10 times or as many epochs if < 10\n        if i% math.ceil(num_epochs / 10) == 0:\n            print(f\"Epoch {i:4d}: Cost {J_history[-1]}   \")\n        \n    return w, b, J_history\n    \n#Dataset\nX_train = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\ny_train = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1])\nw_train  = np.zeros_like(X_train[0])\nb_train  = 0.\nalpha = 0.05\nepochs = 90000\nw, b, _ = gradient_descent(X_train, y_train, w_train, b_train, alpha, epochs) \nprint(f\"\\nUpdated parameters: w:{w}, b:{b}\")\n#Study - 5h\nx_to_predict = np.array([[5]])\n\nprobability = predict_logistic(x_to_predict, w, b)\n\nprint(f\"The predicted probability for x = 5 is: {probability[0]}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch    0: Cost 0.5153904365743195   \nEpoch 9000: Cost 0.05410248584625591   \nEpoch 18000: Cost 0.03644581170979147   \nEpoch 27000: Cost 0.027991143798729672   \nEpoch 36000: Cost 0.02281173904546763   \nEpoch 45000: Cost 0.01926742724277765   \nEpoch 54000: Cost 0.016677366844472807   \nEpoch 63000: Cost 0.014698342133587908   \nEpoch 72000: Cost 0.013135907329703813   \nEpoch 81000: Cost 0.011870826396235896   \n\nUpdated parameters: w:[5.80359202], b:-20.17150597246376\nThe predicted probability for x = 5 is: 0.9998561296500099\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}